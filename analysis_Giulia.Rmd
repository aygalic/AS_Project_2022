---
title: "Analisi del dataset"
author: "luca"
date: '2022-03-18'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1.IMPORT DATASET

```{r cars}
data_patient= read.delim(file.path("Dataset", "data_clinical_patient.txt"), header = TRUE, comment.char = '#')

data_treatment_auc= read.delim(file.path("Dataset", 'data_drug_treatment_auc.txt'), header = TRUE, comment.char = '#')

data_sample= read.delim(file.path("Dataset",'data_clinical_sample.txt'), header = TRUE, comment.char = '#')

```


## Selecting Invasive Breast Carcinoma data

```{r}
data_sample_breast = data_sample[data_sample$"CANCER_TYPE_DETAILED"=="Invasive Breast Carcinoma",]
selected_cells = data_sample_breast$SAMPLE_ID
selected_cells = na.omit(selected_cells) #57 values

indexes = match(selected_cells, colnames(data_treatment_auc))
reduced_data_treatment= data_treatment_auc[, c(na.omit(indexes))]
rownames(reduced_data_treatment) <- data_treatment_auc$ENTITY_STABLE_ID
```

## NA
```{r}

col_names = reduced_data_treatment[1,]
perc_NA_col = colMeans(is.na(reduced_data_treatment))*100

#for(i in 1:45){
  #cat("For the cell line", col_names[i+1], "there is", perc_NA_col[i+1], "% of missing values \n")
#}

row_names = reduced_data_treatment[,1]
perc_NA_row = rowMeans(is.na(reduced_data_treatment))*100

#for(i in 1:266){
  #cat("For the drug", row_names[i], "there is", perc_NA_row[i], "% of missing values \n")
#}

mean(perc_NA_row) 
mean(is.na(reduced_data_treatment))*100 # 20%


#I think we should start deleting rows since a drug that has been tested on few patients it's probably not worth to be studied 

#I start with 50% tolerance on the rows
count=sum(perc_NA_row>50)

max_perc_row = max(perc_NA_row)
while(max_perc_row > 50)
{
  index = match(max_perc_row, perc_NA_row)
  reduced_data_treatment = reduced_data_treatment[-c(index),]
  perc_NA_row = perc_NA_row[-c(index)]
  max_perc_row = max(perc_NA_row)
}
#from 266 to 224 rows

#update the percentages
perc_NA_col = colMeans(is.na(reduced_data_treatment))*100

#50% tolerance on the columns
max_perc_col = max(perc_NA_col)
while(max_perc_col > 50)
{
  index = match(max_perc_col, perc_NA_col)
  reduced_data_treatment = reduced_data_treatment[,-c(index)]
  perc_NA_col = perc_NA_col[-c(index)]
  max_perc_col = max(perc_NA_col)
}

#from 46 to 43 columns

#update the percentages for rows
perc_NA_row = rowMeans(is.na(reduced_data_treatment))*100

mean(perc_NA_row)
mean(perc_NA_col)
mean(is.na(reduced_data_treatment))*100 
#it's 7,82 % for both (it must be)

#I think I should put more restrictions on the rows

#10% tolerance on the rows and I see if the mean percentage is 
max_perc_row = max(perc_NA_row)
while(max_perc_row > 10 && mean(perc_NA_row) > 2)
{
  index = match(max_perc_row, perc_NA_row)
  reduced_data_treatment = reduced_data_treatment[-c(index),]
  perc_NA_row = perc_NA_row[-c(index)]
  max_perc_row = max(perc_NA_row)
}

#from 224 to 143 rows

#update the percentages for columns
perc_NA_col = colMeans(is.na(reduced_data_treatment))*100

mean(perc_NA_row)
mean(perc_NA_col)
mean(is.na(reduced_data_treatment))*100

# Substitute NA value with 0.5
reduced_data_treatment[is.na(reduced_data_treatment)]= 0.5
sum(is.na(reduced_data_treatment))
```

## HEATMAP
```{r}
library(heatmaply)
heatmaply(data.matrix(reduced_data_treatment)) # interactive heat map
```

## HEATMAP 2
```{r}
# interactive ticks but no dendogram
heatmaply(data.matrix(reduced_data_treatment),
          dynamicTicks = TRUE, 
          show_dendrogram=FALSE, 
          margins = c(50,50,50,50)) 

# no tickname
heatmaply(data.matrix(reduced_data_treatment),
          showticklabels = FALSE, 
          margins = c(50,50,50,50)) 



```



## PCA
```{r}
pc <- princomp(reduced_data_treatment,scores=T)
summary(pc)

# Visualization

dev.new()
plot(pc, las=2, main='Principal components', ylim=c(0,1))
barplot(sapply(reduced_data_treatment,sd)^2, las=2, main='Original Variables', ylim=c(0,0.1), ylab='Variances',col='pink')
plot(cumsum(pc$sd^2)/sum(pc$sd^2), type='b', xlab='number of components', 
     ylab='contribution to the total variance', ylim=c(0,1))
abline(h=0.75, col='red')
#biplot(pc) # Viene male
dev.off()
```

## Scores and loadings
```{r}
load.treatment <- pc$loadings
scores.treatment <- pc$scores

load.treatment[load.treatment>0.5]
dev.new()
for(i in 1:3)barplot(load.treatment[,i], ylim = c(-0.5, 0.5), main=paste('Loadings PC ',i,sep=''))
for(i in 1:3)barplot(scores.treatment[,i], ylim = c(-1.5, 1.5), main=paste('Scores PC ',i,sep=''))
dev.off()
```

QUESTION: Do you know if there is a way to select only the cell line for loadings and treatment for scores with absolute value bigger than N? 

## Plot first 3 PC
```{r}
M <- colMeans(reduced_data_treatment)
library(rgl)
open3d()
points3d(reduced_data_treatment, asp=1, size=4)
axes3d()


PC123 <- NULL
for(i in 1:142)PC123 <- rbind(PC123, pc$loadings[,1]*pc$scores[i,1] + pc$loadings[,2]*pc$scores[i,2] + pc$loadings[,3]*pc$scores[i,3] + M)
points3d(PC123, col='red', size=6)

for(i in 1:142)lines3d(rbind(reduced_data_treatment[i,], PC123[i,]),col='blue')

lines3d(rbind(M + 2*pc$sdev[1] * pc$loadings[,1], M - 2*pc$sdev[1] * pc$loadings[,1]), col='forestgreen',lwd=2) 
lines3d(rbind(M + 2*pc$sdev[2] * pc$loadings[,2], M - 2*pc$sdev[2] * pc$loadings[,2]), col='forestgreen',lwd=2) 
lines3d(rbind(M + 2*pc$sdev[3] * pc$loadings[,3], M - 2*pc$sdev[3] * pc$loadings[,3]), col='forestgreen',lwd=2)

# Problem: Tha difference between the length of the green line is high becouse we have very different values for sdev

library(pca3d)
pca3d(pc)
```

```{r}
for(i in 1:143){
  if(gr[i]>0.8)
    color[i]='red'
  else 
    color[i]='blue'
}
  
 
library(pca3d)
pca3d(pc, col= color)
```

```{r}
#Principal component bigger than 0 and mean of the treatment >0.8
s= pc$scores
true_good=0
false_good=0
true_bad=0
false_bad=0
for(i in 1:143){
  if(s[i,1]<=1){
    if(gr[i]>0.8)
      true_good= true_good +1
    else false_good= false_good+1
  }
  else{
    if(gr[i]>0.8)
      false_bad= false_bad +1
    else true_bad= true_bad +1
  }
}

for(i in 1:143){
  if(s[i,2]>=-1){
    if(gr[i]>0.8)
      true_good= true_good +1
    else false_good= false_goo---.d+1
  }
  else{
    if(gr[i]>0.8)
      false_bad= false_bad +1
    else true_bad= true_bad +1
  }
}

p= ((true_good+true_bad)/143)*100
```


# Transpose

```{r}
reduced_data_treatment_T= t(reduced_data_treatment)

pcT <- princomp(reduced_data_treatment_T,scores=T) # More units than variables we cannot use it

```

## All dataset

```{r}
# Remove useless columns
data_treatment_auc$NAME <- NULL
data_treatment_auc$URL <- NULL
data_treatment_auc$DESCRIPTION <- NULL
rownames(data_treatment_auc) <- data_treatment_auc$ENTITY_STABLE_ID
data_treatment_auc$ENTITY_STABLE_ID <- NULL

```

```{r}
#Remove NA
mean(is.na(data_treatment_auc))*100 # 20.85778%

col_names = data_treatment_auc[1,]
perc_NA_col = colMeans(is.na(data_treatment_auc))*100


row_names = reduced_data_treatment[,1]
perc_NA_row = rowMeans(is.na(data_treatment_auc))*100

sum(perc_NA_row>50)
max_perc_row = max(perc_NA_row)
while(max_perc_row > 50)
{
  index = match(max_perc_row, perc_NA_row)
  data_treatment_auc = data_treatment_auc[-c(index),]
  perc_NA_row = perc_NA_row[-c(index)]
  max_perc_row = max(perc_NA_row)
}

# 266 --> 218

perc_NA_col = colMeans(is.na(data_treatment_auc))*100

#50% tolerance on the columns
max_perc_col = max(perc_NA_col)
while(max_perc_col > 50)
{
  index = match(max_perc_col, perc_NA_col)
  data_treatment_auc = data_treatment_auc[,-c(index)]
  perc_NA_col = perc_NA_col[-c(index)]
  max_perc_col = max(perc_NA_col)
}

#from 1065 --> 978

#update the percentages for rows
perc_NA_row = rowMeans(is.na(data_treatment_auc))*100

mean(perc_NA_row)
mean(perc_NA_col)

sum(perc_NA_row>10)

max_perc_row = max(perc_NA_row)
while(max_perc_row > 8)
{
  index = match(max_perc_row, perc_NA_row)
  data_treatment_auc = data_treatment_auc[-c(index),]
  perc_NA_row = perc_NA_row[-c(index)]
  max_perc_row = max(perc_NA_row)
}

mean(perc_NA_row)
mean(perc_NA_col)
mean(is.na(data_treatment_auc))*100 # 3.466

```
We stop here becouse with the complete dataset at this moment we have less treatment and more NA value.
We can perform PCA and so on but maybe this isn't the best way.

